* How to reduce a classification problem into a regression problem? [[machine learning]]
[[https://datascience.stackexchange.com/questions/39264/how-does-sigmoid-activation-work-in-multi-class-classification-problems][How does Sigmoid activation work in multi-class classification problems]]

The difference between 2-classification problem and regression problem is that the range of 2-classification (say \( \{1, 2\} \) problem is discrete. We can use sigmoid function to smoothen the function from features to the category of the classification. And then use regression techniques to solve this problme.
For a multiclass classification problem, say we have \( n \) categories, we can reduce this classification to a \( n \) regression problem. Each of the these regression functions take value \( 1 \) on one category and \( 0 \) on all other categories. There are \( n \) regression functions in total. These functions give a rough probability of whether this sample belong to the corresponding category (every value greater than 0.5 means belonging). We can take maxium of these sigmoid outputs. For mutually exclusive classification task, Softmax function is more appropirate.

[[https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb][Jupyter Notebook Viewer]]
#+BEGIN_QUOTE
Intuitively, the softmax function really wants to pick one class among the others, so it's ideal for training a classifier when we know each picture has a definite label. (Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn't recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.)
#+END_QUOTE
*