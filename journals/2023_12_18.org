* When to use weight decay to avoid overfitting? [[machine learning]]
In [[https://github.com/fastai/fastbook/blob/master/08_collab.ipynb][Collaborative Filtering Deep Dive]], the author used weight decay to prevent overfitting. How does the author know to do this?
* How to interpret the principal component analysis result of the emdding matrix of the top movies? [[principal component analysis]]
[[https://github.com/fastai/fastbook/blob/master/08_collab.ipynb][Collaborative Filtering Deep Dive]]
#+BEGIN_SRC python 
#hide_input
#id img_pca_movie
#caption Representation of movies based on two strongest PCA components
#alt Representation of movies based on two strongest PCA components
g = ratings.groupby('title')['rating'].count()
top_movies = g.sort_values(ascending=False).index.values[:1000]
top_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])
movie_w = learn.model.movie_factors[top_idxs].cpu().detach()
movie_pca = movie_w.pca(3)
fac0,fac1,fac2 = movie_pca.t()
idxs = list(range(50))
X = fac0[idxs]
Y = fac2[idxs]
plt.figure(figsize=(12,12))
plt.scatter(X, Y)
for i, x, y in zip(top_movies[idxs], X, Y):
    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)
plt.show()
#+END_SRC
* Metrics used in machine learning. [[machine learning]]
** Binary classification
*** accurancy
accurancy = \( \frac{\#\{ \text{correct predictions} \}}{\#\{ \text{predictions} \} }\)
*** precision
precision = \( \frac{\#\{ \text{TP} \}}{\#( \text{TP} \cup \text{FP} )  }\)
*** recall
rercall = \( \frac{\#\{ \text{TP} \}}{\#( \text{TP} \cup \text{FN} )  }\)
** Multi-class classification
* There is a DecisionTreeRegressor in scikit-learn? How can we use decision tree for regression tasks? What is the staticstical value that we want to minimize while divide a set into two subsets? This value should represent how homogenous the two subsets are. [[decision tree]]
[[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html][sklearn.tree.DecisionTreeRegressor — scikit-learn 1.3.2 documentation]]
#+BEGIN_QUOTE
criterion{“squared_error”, “friedman_mse”, “absolute_error”, “poisson”}, default=”squared_error”
The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits.
#+END_QUTOE