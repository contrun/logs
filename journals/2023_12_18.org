* When to use weight decay to avoid overfitting? [[machine learning]]
In [[https://github.com/fastai/fastbook/blob/master/08_collab.ipynb][Collaborative Filtering Deep Dive]], the author used weight decay to prevent overfitting. How does the author know to do this?
* How to interpret the principal component analysis result of the emdding matrix of the top movies? [[principal component analysis]]
[[https://github.com/fastai/fastbook/blob/master/08_collab.ipynb][Collaborative Filtering Deep Dive]]
#+BEGIN_SRC python 
#hide_input
#id img_pca_movie
#caption Representation of movies based on two strongest PCA components
#alt Representation of movies based on two strongest PCA components
g = ratings.groupby('title')['rating'].count()
top_movies = g.sort_values(ascending=False).index.values[:1000]
top_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])
movie_w = learn.model.movie_factors[top_idxs].cpu().detach()
movie_pca = movie_w.pca(3)
fac0,fac1,fac2 = movie_pca.t()
idxs = list(range(50))
X = fac0[idxs]
Y = fac2[idxs]
plt.figure(figsize=(12,12))
plt.scatter(X, Y)
for i, x, y in zip(top_movies[idxs], X, Y):
    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)
plt.show()
#+END_SRC
* Metrics used in machine learning. [[machine learning]]
** Binary classification
*** accurancy
accurancy = \( \frac{\#\{ \text{correct predictions} \}}{\#\{ \text{predictions} \} }\)
*** precision
precision = \( \frac{\#\{ \text{TP} \}}{\#( \text{TP} \cup \text{FP} )  }\)
*** recall
precision = \( \frac{\#\{ \text{TP} \}}{\#( \text{TP} \cup \text{FP} )  }\)
** Multi-class classification