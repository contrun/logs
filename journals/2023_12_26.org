* A mathematical theory of collaborative filtering? Collaborative filtering is a machine learning method that is used to recommend related items to users. Users have preferences over latent factors and items have properties of latent factors. Each users can be represented as a vector of weights of latent factors, while each item can be represented as a vector of weights of the dual of latent factors. The dot product can be viewed as user's preferences over that item. The norm of the user and item can be viewed as the intrinsic value of the user or item. The collaborative filtering process is th find out similar users and thus find out items some user may like. There may be some way to filter out obviously wrong items for a specific user. [[collaborative filtering]]
* How do we find out the items that users may like in collaborative filtering? Do we need to calculate the dot products of each item and each user one by one ? What are the ways to improve efficiency in real world recommendations systems? [[collaborative filtering]]
* How to spot overfitting? Give some clear indication of overfishing in machine learning. [[machine learning]]
* Entity Embeddings of Categorical Variables. What they found was quite amazing, and illustrates their second key insight: the embedding transforms the categorical variables into inputs that are both continu‐ ous and meaningful./ [[TODO]]
* What is the best way to divide data into two sets by a unordered categorical  feature with high cardinality in decision tree algorithm? We can take all subset partitions of the categorical features, but this seems to be grossly inefficient.  The standard approach for nominal predictors is to consider all 2k − 1 − 1 2-partitions of the k predictor categories. However, this exponential relationship produces a large number of potential splits to be evaluated, increasing computational complexity and restricting the possible number of categories in most implementations. For binary clas‐ sification and regression, it was shown that ordering the predictor categories in each split leads to exactly the same splits as the standard approach. This reduces computa‐ tional complexity because only k − 1 splits have to be considered for a nominal predic‐ tor with k categories. [[TODO]] [[decision tree]]
* What is an OOB score in random forests? [[TODO]]
* Finding out of domain data with random forests. We simply and a column to indicate whether this column corresponds to test data or validation data, and then we find out which features are most important. The important features may represent some characteristics of the out of domain data.