* How to encode categorical values in machine learning? [[machine learning]]
[[https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02][All about Categorical Variable Encoding | by Baijayanta Roy | Towards Data Science]]
Say that a field F has a few alternative values (f_1, f_2, f_3), we normally encode this field by create fields like has_field_1, has_field_2 (has_field_3 is ignored as it can be inferred from has_field_1 and has_field_2), the value of has_field_1 and has_field_2 are 1 or 0 to signify the existence of f_1 and f_2. This method is also called [[https://en.wikipedia.org/wiki/One-hot][One-hot encoding]].
* Why is pytorch's way to calculate derivative so peculiar? [[pytorch]]
Here is how a function ~f~'s derivative calculated in [[https://nbviewer.org/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb][Jupyter Notebook Viewer]]
#+BEGIN_SRC python
def mse(preds, targets): return ((preds-targets)**2).mean()
time = torch.arange(0,20).float(); time
params = torch.randn(3).requires_grad_()
preds = f(time, params)
loss = mse(preds, speed)
loss.backward()
params.grad
#+END_SRC

It seems that the purpose of this it to keep track the derivative even if ~f~ is a compose of serveral functions.
#+BEGIN_QUOTE
Understanding this bit depends on remembering recent history. To calculate the gradients we call ~backward~ on the ~loss~. But this ~loss~ was itself calculated by ~mse~, which in turn took ~preds~ as an input, which was calculated using ~f~ taking as an input ~params~, which was the object on which we originally called ~requires_grad_~â€”which is the original call that now allows us to call ~backward~ on ~loss~. This chain of function calls represents the mathematical composition of functions, which enables PyTorch to use calculus's chain rule under the hood to calculate these gradients.
#+END_QUOTE
* Why does the ~backward~ method of pytorch add gradient to stored gradient? [[pytorch]]
[[https://nbviewer.org/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb][Jupyter Notebook Viewer]]
#+BEGIN_QUOTE
The gradients have changed! The reason for this is that `loss.backward` actually /*adds*/ the gradients of `loss` to any gradients that are currently stored. So, we have to set the current gradients to 0 first.
#+END_QUOTE