* Why do we need to take exponential in softmax? [[deep learning]]
The definition of softmax is \( \operatorname{softmax}(x_i) = \frac{ e^{x_i} }{ \sum_i e^{x_i} } \). Why don't we directly define softmax as \( \operatorname{softmax}(x_i) = \frac{ {x_i} }{ \sum_i {x_i} } \)? Is it used to amplify the difference and smoothen the function?
It seems the reason is the same as why do we use sigmoid to 2-classification problem. In 2-classification case softmax is the same as sigmoid, as \( sigmoid(x) = \frac{e^x}{1+e^x} \).

[[https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb][Jupyter Notebook Viewer]]
#+BEGIN_QUOTE
We could create other functions that have the properties that all activations are between 0 and 1, and sum to 1; however, no other function has the same relationship to the sigmoid function, which we've seen is smooth and symmetric. Also, we'll see shortly that the softmax function works well hand-in-hand with the loss function we will look at in the next section.
#+END_QUOTE

#+BEGIN_QUOTE
The exponential also has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows, well... exponentially), which means that in the softmax, that number will be closer to 1.
#+END_QUOTE
* Why is cross-entropy loss function called cross-entropy? [[deep learning]]
Given the following dataframe (category{1,2} stand for the probability predicted by the model for category one or two, tgt stands fot the correct label for the training data, idx stands for the index of the sample),
#+BEGIN_SRC text
category1  category2   tgt idx	results
0.602469 	0.397531 	0 	0 	0.602469
0.502065 	0.497935 	1 	1 	0.497935
0.133188 	0.866811 	0 	2 	0.133188
0.996640 	0.003360 	1 	3 	0.003360
0.595949 	0.404051 	1 	4 	0.404051
0.366118 	0.633882 	0 	5 	0.366118
#+END_SRC

Here the results is calculated by ~ df['results'] = df.category0 if df.tgt == 0 else df.category1 ~. The cross entropy loss is calculated as \( \frac{\sum_{r \in results} -\log(\mathrm{r})}{\operatorname{len}(results)} \). The question is that why is it called cross entropy?
* How does the learning rate selection algorithm described in fastbook work? [[deep learning]]
[[https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb][Jupyter Notebook Viewer]]
[[https://ieeexplore.ieee.org/abstract/document/7926641][Cyclical Learning Rates for Training Neural Networks | IEEE Conference Publication | IEEE Xplore]]

#+BEGIN_QUOTE
Our advice is to pick either:

    One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)
    The last point where the loss was clearly decreasing
#+END_QUOTE
* Why should we use lower learning rates in early layers and high learning rates in late layers? This is called ~discriminative learning rates~. [[deep learning]]
[[https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb][Jupyter Notebook Viewer]]
#+BEGIN_QUOTE
Like many good ideas in deep learning, it is extremely simple: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers). The idea is based on insights developed by Jason Yosinski, who showed in 2014 that with transfer learning different layers of a neural network should train at different speeds
#+END_QUOTE
* Early stopping in deep learning training. [[deep learning]]
[[https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb][Jupyter Notebook Viewer]]
#+BEGIN_QUOTE 
Before the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy out of all of the models saved in each epoch. This is known as early stopping.
#+END_QUOTE
* Make machine learning algorithms more effective when there are not a huge amount of data, or, create a single strong learner from a set of weak learners?". [[machine learning]]
** Random forest bagging [[random forest]]
Make a few decision trees and take the most predicted category as output.
** Mixup
[[https://openreview.net/forum?id=r1Ddp1-Rb][mixup: Beyond Empirical Risk Minimization | OpenReview]]
** Ensemble learning
* Techniques to avoid overfitting in machine learning. [[machine learning]]
** label smoothing
- [[https://paperswithcode.com/method/label-smoothing][Label Smoothing Explained | Papers With Code]]
- [[1906.02629] When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)