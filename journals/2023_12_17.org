* Why do we need to take exponential in softmax? [[deep learning]]
The definition of softmax is \( \operatorname{softmax}(x_i) = \frac{ e^{x_i} }{ \sum_i e^{x_i} } \). Why don't we directly define softmax as \( \operatorname{softmax}(x_i) = \frac{ {x_i} }{ \sum_i {x_i} } \)? Is it used to amplify the difference and smoothen the function?
It seems the reason is the same as why do we use sigmoid to 2-classification problem. In 2-classification case softmax is the same as sigmoid, as \( sigmoid(x) = \frac{e^x}{1+e^x} \).
*